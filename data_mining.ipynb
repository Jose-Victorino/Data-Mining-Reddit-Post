{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce4cdce",
   "metadata": {},
   "source": [
    "### SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866e8c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit mining using PRAW\n",
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "  client_id='6NE27-qV7tGX07FuBfik3w',\n",
    "  client_secret='fauqCWKne1OXmmtzPIABkNW57DFyZA',\n",
    "  user_agent='Comment scraper'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330968e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import html\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "subreddit_names = ['Anxiety', 'mentalhealth']\n",
    "posts_data = []\n",
    "total_comments = 0\n",
    "target_comments = 5000\n",
    "mod_keywords = ['announcement', 'rules', 'mod', 'moderator', 'admin', 'clarification', 'faq', 'meta', 'update', 'policy']\n",
    "\n",
    "def is_mod_post(post):\n",
    "  if post.distinguished in ['moderator', 'admin']:\n",
    "    return True\n",
    "  text = (post.title + \" \" + post.selftext).lower()\n",
    "  return any(keyword in text for keyword in mod_keywords)\n",
    "\n",
    "def is_mod_comment(comment):\n",
    "  if comment.distinguished in ['moderator', 'admin']:\n",
    "    return True\n",
    "  text = comment.body.lower()\n",
    "  return any(keyword in text for keyword in mod_keywords)\n",
    "\n",
    "def clean_text(text):\n",
    "  if not text:\n",
    "    return \"\"\n",
    "  text = html.unescape(text)                                                  # Decode HTML entities\n",
    "  text = re.sub(r'\\[deleted\\]|\\[removed\\]', '', text, flags = re.IGNORECASE)  # remove [deleted]/[removed]\n",
    "  text = re.sub(r'http\\S+|www\\S+', '', text)                                  # Remove URLs\n",
    "  text = re.sub(r'&\\w+;', '', text)                                           # Remove encoded HTML symbols\n",
    "  text = re.sub(r'u/\\w+|r/\\w+', '', text)                                     # Remove mentions\n",
    "  text = re.sub(r'>.*\\n?', '', text)                                          # Remove blockquotes\n",
    "  text = re.sub(r'\\*+', '', text)                                             # Remove markdown asterisks\n",
    "  text = re.sub(r'[\\r\\n\\t]', ' ', text)                                       # Normalize whitespace\n",
    "  text = re.sub(r'[^a-zA-Z\\s\\.,!?\\'\":;()\\[\\]{}\\-]', '', text)                 # Remove numbers, unicode, & symbols\n",
    "  text = re.sub(r'[^\\x00-\\x7F]+', '', text)                                   # Remove non-ASCII characters\n",
    "  text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)                                # Remove spaces before punctuation\n",
    "  text = re.sub(r'([!?])\\1{2,}', r'\\1\\1\\1', text)                             # Limit repeated punctuation to max 3\n",
    "  text = re.sub(r'(\\.\\s*){2,}', '...', text)                                  # Normalize ellipses\n",
    "  text = re.sub(r'\\.{4,}', '...', text)                                       # Normalize ellipses\n",
    "  text = re.sub(r'\\s{2,}', ' ', text)                                         # Collapse multiple spaces\n",
    "  return text.strip().lower()\n",
    "\n",
    "def is_english(text):\n",
    "  try:\n",
    "    if len(text) < 20:\n",
    "      return False\n",
    "    return detect(text) == 'en'\n",
    "  except LangDetectException:\n",
    "    return False\n",
    "\n",
    "for sub_name in subreddit_names:\n",
    "  sub_comments = 0\n",
    "  \n",
    "  for submission in reddit.subreddit(sub_name).new(limit = None):\n",
    "    if sub_comments >= target_comments / len(subreddit_names):\n",
    "      break\n",
    "\n",
    "    if is_mod_post(submission):\n",
    "      continue\n",
    "\n",
    "    submission.comments.replace_more(limit = 0)\n",
    "    comments = []\n",
    "\n",
    "    for comment in submission.comments.list():\n",
    "      if is_mod_comment(comment):\n",
    "        continue\n",
    "\n",
    "      clean_body = clean_text(comment.body)\n",
    "\n",
    "      if not is_english(clean_body):\n",
    "        continue\n",
    "\n",
    "      comments.append({\n",
    "        'comment_id': comment.id,\n",
    "        'body': clean_body,\n",
    "        'author': comment.author.name if comment.author else \"[deleted]\",\n",
    "        'author_role': comment.distinguished,\n",
    "        'score': comment.score,\n",
    "        'created_utc': comment.created_utc,\n",
    "        'is_submitter': comment.is_submitter,\n",
    "        'parent_id': comment.parent_id,\n",
    "        'permalink': comment.permalink\n",
    "      })\n",
    "\n",
    "    if comments:\n",
    "      sub_comments += len(comments)\n",
    "      total_comments += len(comments)\n",
    "\n",
    "      post = {\n",
    "        'post_id': submission.id,\n",
    "        'title': clean_text(submission.title),\n",
    "        'selftext': clean_text(submission.selftext),\n",
    "        'author': submission.author.name if submission.author else \"[deleted]\",\n",
    "        'author_role': submission.distinguished,\n",
    "        'score': submission.score,\n",
    "        'upvote_ratio': submission.upvote_ratio,\n",
    "        # 'url': submission.url,\n",
    "        'created_utc': submission.created_utc,\n",
    "        'num_comments': len(comments),\n",
    "        'subreddit': submission.subreddit.display_name,\n",
    "        'flair': submission.link_flair_text,\n",
    "        'is_self': submission.is_self,\n",
    "        'nsfw': submission.over_18,\n",
    "        'permalink': submission.permalink,\n",
    "        'comments': comments\n",
    "      }\n",
    "      posts_data.append(post)\n",
    "\n",
    "  print(f\"{sub_comments} comments collected from r/{sub_name}\")\n",
    "\n",
    "with open('raw_posts.json', 'w', encoding='utf-8') as f:\n",
    "  json.dump(posts_data, f, indent = 2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nScraped {len(posts_data)} posts with {total_comments} comments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25152a39",
   "metadata": {},
   "source": [
    "### DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b43f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('raw_posts.json', 'r', encoding='utf-8') as f:\n",
    "  raw_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4d78e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "cleaned_data = []\n",
    "total_cleaned_posts = 0\n",
    "total_cleaned_comments = 0\n",
    "mod_keywords = ['announcement', 'rules', 'mod', 'moderator', 'admin', 'clarification', 'faq', 'meta', 'update', 'policy']\n",
    "\n",
    "def format_datetime(utc_timestamp):\n",
    "  return datetime.fromtimestamp(utc_timestamp, tz=timezone.utc).isoformat()\n",
    "\n",
    "def is_meaningful(text):\n",
    "  if len(text) < 20:                      # Remove short comments\n",
    "    return False\n",
    "  if not any(c.isalpha() for c in text):  # Must contain alphabetic characters\n",
    "    return False\n",
    "  if len(text) > 2000:                    # remove very long comments \n",
    "    return False\n",
    "  return True\n",
    "\n",
    "for post in raw_data:\n",
    "  cleaned_post = {\n",
    "    'post_id': post['post_id'],\n",
    "    'title': post['title'],\n",
    "    'selftext': post['selftext'],\n",
    "    'author': post['author'],\n",
    "    'author_role': post['author_role'],\n",
    "    'score': post['score'],\n",
    "    'upvote_ratio': post['upvote_ratio'],\n",
    "    'created_utc': format_datetime(post['created_utc']),\n",
    "    'num_comments': 0,\n",
    "    'subreddit': post['subreddit'],\n",
    "    'flair': post['flair'],\n",
    "    'is_self': post['is_self'],\n",
    "    'nsfw': post['nsfw'],\n",
    "    'permalink': post['permalink'],\n",
    "    'comments': []\n",
    "  }\n",
    "\n",
    "  for comment in post['comments']:\n",
    "    if comment['body'] == \"\":\n",
    "      continue\n",
    "    if comment['author'] == \"[deleted]\":\n",
    "      continue\n",
    "    if not is_meaningful(comment['body']):\n",
    "      continue\n",
    "\n",
    "    cleaned_comment = {\n",
    "      'comment_id': comment['comment_id'],\n",
    "      'body': comment['body'],\n",
    "      'author': comment['author'],\n",
    "      'author_role': comment['author_role'],\n",
    "      'score': comment['score'],\n",
    "      'created_utc': format_datetime(comment['created_utc']),\n",
    "      'is_submitter': comment['is_submitter'],\n",
    "      'parent_id': comment['parent_id'],\n",
    "      'permalink': comment['permalink']\n",
    "    }\n",
    "    cleaned_post['comments'].append(cleaned_comment)\n",
    "\n",
    "  if cleaned_post['comments']:\n",
    "    cleaned_post['num_comments'] = len(cleaned_post['comments'])\n",
    "    total_cleaned_posts += 1\n",
    "    total_cleaned_comments += len(cleaned_post['comments'])\n",
    "    cleaned_data.append(cleaned_post)\n",
    "\n",
    "with open('cleaned_posts.json', 'w', encoding='utf-8') as f:\n",
    "  json.dump(cleaned_data, f, indent = 2, ensure_ascii = False)\n",
    "\n",
    "print(f\"Data cleaned. {total_cleaned_posts} posts remaining with {total_cleaned_comments} comments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77acf31d",
   "metadata": {},
   "source": [
    "### SQL LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84624944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "  host = \"127.0.0.1\",\n",
    "  user = \"postgres\",\n",
    "  password = \"SqlPassword\",\n",
    "  dbname = \"reddit_data\",\n",
    "  port = \"5432\"\n",
    ")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81babb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('cleaned_posts.json', 'r', encoding = 'utf-8') as f:\n",
    "  data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7999c8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "def unique_topic(flair):\n",
    "  cur.execute(\"\"\"\n",
    "    INSERT INTO dim_topic (flair)\n",
    "    VALUES (%s)\n",
    "    ON CONFLICT (flair) DO NOTHING\n",
    "    RETURNING topic_id;\n",
    "  \"\"\", (flair,))\n",
    "  result = cur.fetchone()\n",
    "  if not result:\n",
    "    cur.execute(\"SELECT topic_id FROM dim_topic WHERE flair = %s;\", (flair,))\n",
    "    result = cur.fetchone()\n",
    "  return result[0]\n",
    "\n",
    "def unique_date_id(dt):\n",
    "  cur.execute(\"\"\"\n",
    "    INSERT INTO dim_date (full_date, year, quarter, month, day, weekday)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "    ON CONFLICT (full_date) DO NOTHING\n",
    "    RETURNING date_id;\n",
    "  \"\"\", (dt.date(), dt.year, ((dt.month - 1) // 3 + 1), dt.month, dt.day, dt.strftime('%A')))\n",
    "  result = cur.fetchone()\n",
    "  if not result:\n",
    "    cur.execute(\"SELECT date_id FROM dim_date WHERE full_date = %s;\", (dt.date(),))\n",
    "    result = cur.fetchone()\n",
    "  return result[0]\n",
    "\n",
    "for post in data:\n",
    "  cur.execute(\"\"\"\n",
    "    INSERT INTO dim_posts (post_id, title, selftext, author, author_role, score, upvote_ratio, created_utc, num_comments, subreddit, is_self, nsfw, permalink)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);\n",
    "    \"\"\", (post['post_id'], post['title'], post['selftext'], post['author'], post['author_role'], post['score'], post['upvote_ratio'], post['created_utc'], post['num_comments'], post['subreddit'], post['is_self'], post['nsfw'], post['permalink'])\n",
    "  )\n",
    "\n",
    "  topic_id = unique_topic(post['flair'])\n",
    "\n",
    "  for comment in post['comments']:\n",
    "    cur.execute(\"\"\"\n",
    "      INSERT INTO dim_comment (comment_id, author, author_role, body, score, created_utc, is_submitter, parent_id, permalink)\n",
    "      VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s);\n",
    "      \"\"\", (comment['comment_id'], comment['author'], comment['author_role'], comment['body'], comment['score'], comment['created_utc'], comment['is_submitter'], comment['parent_id'], comment['permalink'])\n",
    "    )\n",
    "\n",
    "    dt = datetime.fromisoformat(comment['created_utc'])\n",
    "    date_id = unique_date_id(dt)\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "      INSERT INTO fact_table (comment_id, post_id, topic_id, date_id, is_submitter, comment_score, comment_length)\n",
    "      VALUES (%s, %s, %s, %s, %s, %s, %s);\n",
    "      \"\"\", (comment['comment_id'], post['post_id'], topic_id, date_id, comment['is_submitter'], comment['score'], len(comment['body']))\n",
    "    )\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb361c1f",
   "metadata": {},
   "source": [
    "### TOPIC MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558988be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "  host = \"127.0.0.1\",\n",
    "  user = \"postgres\",\n",
    "  password = \"SqlPassword\",\n",
    "  dbname = \"reddit_data\",\n",
    "  port = \"5432\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Sample: Load comments\n",
    "df = pd.read_sql_query(\"SELECT comment_id, body FROM dim_comment;\", conn)\n",
    "df.dropna(subset=[\"body\"], inplace=True)\n",
    "\n",
    "# Lemmatize and clean\n",
    "def preprocess(text):\n",
    "  doc = nlp(text)\n",
    "  return \" \".join([\n",
    "    token.lemma_ for token in doc \n",
    "    if not token.is_stop and token.is_alpha and token.pos_ != \"PRON\"\n",
    "  ])\n",
    "\n",
    "# Apply preprocessing\n",
    "df[\"cleaned_body\"] = df[\"body\"].apply(preprocess)\n",
    "documents = df[\"cleaned_body\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "faf39349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic(language=\"english\", nr_topics=3)  # Reduce to 3 topics\n",
    "topics, probs = topic_model.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44b2f123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic  Count                         Name  \\\n",
      "0     -1   3800    -1_feel_like_anxiety_help   \n",
      "1      0   1080     0_like_people_know_think   \n",
      "2      1    110  1_anxiety_attack_panic_feel   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [feel, like, anxiety, help, thing, time, know,...   \n",
      "1  [like, people, know, think, good, thank, life,...   \n",
      "2  [anxiety, attack, panic, feel, like, help, tim...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [ill honest experience anxiety shock deeply bo...  \n",
      "1  [hey happen time feel lose disconnected time g...  \n",
      "2  [bad thing anxiety rid severe case chronic anx...  \n",
      "Topic 0 keywords: [('feel', np.float64(0.05193557742878411)), ('like', np.float64(0.049319934840007115)), ('anxiety', np.float64(0.042647111034908626)), ('help', np.float64(0.04087425041857313)), ('thing', np.float64(0.035242321498194645)), ('time', np.float64(0.035112777569198075)), ('know', np.float64(0.03344455598681058)), ('people', np.float64(0.032969603712441675)), ('think', np.float64(0.032901679918149906)), ('try', np.float64(0.03194295721815392))]\n",
      "Topic 1 keywords: [('like', np.float64(0.046230453833830225)), ('people', np.float64(0.044458364667539774)), ('know', np.float64(0.03861711967472433)), ('think', np.float64(0.03807269285481601)), ('good', np.float64(0.03601418595238886)), ('thank', np.float64(0.03568121578171338)), ('life', np.float64(0.03513844213629043)), ('time', np.float64(0.03510876266847068)), ('go', np.float64(0.03276677130101552)), ('thing', np.float64(0.031714462606521866))]\n",
      "Topic 2 keywords: [('anxiety', np.float64(0.12896670307698482)), ('attack', np.float64(0.06707691703208182)), ('panic', np.float64(0.06576771565899732)), ('feel', np.float64(0.06310315594460884)), ('like', np.float64(0.05385703247571478)), ('help', np.float64(0.04023571759793021)), ('time', np.float64(0.03967890365298417)), ('day', np.float64(0.03787199912106454)), ('symptom', np.float64(0.037538452071051207)), ('bad', np.float64(0.037395516814132564))]\n"
     ]
    }
   ],
   "source": [
    "# Inspect topic keywords\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info)\n",
    "\n",
    "# Example: get keywords for each topic\n",
    "for i in range(3):\n",
    "  print(f\"Topic {i} keywords:\", topic_model.get_topic(i - 1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
